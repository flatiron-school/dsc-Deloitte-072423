{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For our modeling steps\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# For demonstrative purposes\n",
    "from scipy.special import logit, expit\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "- Describe conceptually the need to move beyond linear regression\n",
    "- Explain the form of logistic regression\n",
    "- Explain how to interpret logistic regression coefficients\n",
    "- Use logistic regression to perform a classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "[Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model) has a nice description of the need to move beyond linear regression for certain sorts of modeling problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "Categorizing compared to regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "### Classic example is image classification: dog or cat? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "![Dog wearing knitted cat hat](images/dog_or_cat.jpg)\n",
    "\n",
    "> **WARNING**\n",
    ">\n",
    "> This doesn't refer to the _degree_ of classification but focuses on how likely they are to be correctly classified (subtle)\n",
    "> \n",
    "> _This email is more spammy than the other, but they're both spam_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "### Classification in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "**Classification techniques** are an essential part of machine learning and data mining applications. Most problems in Data Science are classification problems. \n",
    "\n",
    "There are lots of classification algorithms that are available, but we'll focus on logistic regression.\n",
    "\n",
    "We shall focus on binary classification problems, to which logistic regression most immediately applies. Other classification problems handle the cases where multiple classes are present in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Predicting a Categorical Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "Here we have a dataset about glass. Information [here](https://archive.ics.uci.edu/ml/datasets/glass+identification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# glass identification dataset\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "col_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']\n",
    "glass = pd.read_csv(url, names=col_names, index_col='id')\n",
    "glass.sort_values('al', inplace=True)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# types 1, 2, 3 are window glass\n",
    "# types 5, 6, 7 are household glass\n",
    "glass['household'] = glass.glass_type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "Let's change our task, so that we're predicting **household** using **al**. Let's visualize the relationship to figure out how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(glass.al, glass.household)\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household')\n",
    "ax.set_title('Type of Glass as a Function of Aluminum Content');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Using a Regression Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "Let's draw a **regression line**, like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# fit a linear regression model and store the predictions\n",
    "\n",
    "linreg = LinearRegression()\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "linreg.fit(X, y)\n",
    "glass['household_pred'] = linreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# scatter plot that includes the regression line\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(glass.al, glass.household)\n",
    "ax.plot(glass.al, glass.household_pred, color='red')\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "> What are some issues with this graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Interpreting Our Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "If **al=3**, what class do we predict for household? \n",
    "\n",
    "If **al=1.5**, what class do we predict for household? \n",
    "\n",
    "We predict the 0 class for **lower** values of al, and the 1 class for **higher** values of al. What's our cutoff value? Around **al=2**, because that's where the linear regression line crosses the midpoint between predicting class 0 and class 1.\n",
    "\n",
    "Therefore, we'll say that if **household_pred >= 0.5**, we predict a class of **1**, else we predict a class of **0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Logistic Regression Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "Logistic regression can do what we just did.\n",
    "\n",
    "The strategy now is to *generalize* the notion of linear regression; linear regression as we've known it will become a special case. In particular, we'll keep the idea of the regression best-fit line, but now **we'll allow the model to make predictions through some (non-trivial) transformation of the linear predictor**.\n",
    "\n",
    "Let's say we've constructed our best-fit line, i.e. our linear predictor, $\\hat{L} = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$.\n",
    "\n",
    "Consider the following transformation: <br/>\n",
    "$\\large\\hat{y} = \\Large\\frac{1}{1 + e^{-\\hat{L}}} \\large= \\Large\\frac{1}{1 + e^{-(\\beta_0 + ... + \\beta_nx_n)}}$. This is called the **sigmoid function**.\n",
    "\n",
    "We're imagining that $\\hat{L}$ can take any values between $-\\infty$ and $\\infty$.\n",
    "\n",
    "$\\large\\rightarrow$ But what values can $\\hat{y}$ take? What does this function even look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# Let's plot this function here:\n",
    "\n",
    "X = np.linspace(-10, 10, 300)\n",
    "Y = 1 / (1 + np.exp(-X))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(X, Y, 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "This function squeezes our predictions between 0 and 1. And that's why it's so useful for **binary classification problems**.\n",
    "\n",
    "Suppose I'm building a model to predict whether a plant is poisonous or not, based perhaps on certain biological features of its leaves. I'll let '1' indicate a poisonous plant and '0' indicate a non-poisonous plant.\n",
    "\n",
    "Now I'm forcing my predictions to be between 0 and 1, so suppose for test plant $P$ I get some value like 0.19.\n",
    "\n",
    "I can naturally understand this as **the probability that $P$ is poisonous**.\n",
    "\n",
    "If I truly want a binary prediction, I can simply round my score appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Fitting Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "### The Logit Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "We just expressed the form of logistic regression in terms of the sigmoid function: **Our model's predictions ($\\hat{y}$) are not now identical with the values of the best-fit line but rather with the outputs of the sigmoid function, with those best-fit values passed as input.**\n",
    "\n",
    "But we can also describe the best-fit line as a function of $\\hat{y}$, by applying the **inverse of the sigmoid function** to both sides. This inverse function is called the ***logit* function**:\n",
    "\n",
    "$ln(\\frac{y}{1-y}) = \\hat{L} = \\beta_0+\\beta_1x_1 +...+\\beta_nx_n$.\n",
    "\n",
    "This fraction, $\\frac{y}{1-y}$, is the **odds ratio** of y. More on this in the Level-Ups.\n",
    "\n",
    "Let's try applying the logit function to our target and then fitting a linear regression to that. Since the model will be trained not on whether the glass is household but rather on *the logit of this label*, it will also make predictions of the logit of that label. But we can simply apply the sigmoid function to the model's output to get its predictions of whether the glass is household.\n",
    "\n",
    "We can't use the target as is, because the logit of 1 is $\\infty$ and the logit of 0 is $-\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "glass['household'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "logit(glass['household']).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "So we'll make a small adjustment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "target_approx = np.where(glass['household'] == 0, 1e-9, 1-1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "line_to_logit = LinearRegression()\n",
    "\n",
    "X = glass[['al']]\n",
    "y = logit(target_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "line_to_logit.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "final_preds = expit(line_to_logit.predict(X))\n",
    "ax.scatter(X, glass['household'])\n",
    "ax.plot(X, final_preds, 'm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## `sklearn.linear_model.LogisticRegression()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "In general, we should always scale our data when using this class. Scaling is always important for models that include regularization, and scikit-learn's `LogisticRegression()` objects have regularization by default.\n",
    "\n",
    "Here we've forgone the scaling since we only have a single predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# fit a logistic regression model and store the class predictions\n",
    "\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "logreg.fit(X, y)\n",
    "glass['household_pred_class'] = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(glass.al, glass.household)\n",
    "ax.plot(glass.al, glass.household_pred_class, color='red')\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## `.predict()` vs. `.predict_proba()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "Let's checkout some specific examples to make predictions with. We'll use both `predict()` and `predict_proba()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "glass.al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# examine some example predictions\n",
    "\n",
    "print(logreg.predict(glass['al'][22].reshape(1, -1)))\n",
    "print(logreg.predict(glass['al'][185].reshape(1, -1)))\n",
    "print(logreg.predict(glass['al'][164].reshape(1, -1)))\n",
    "print('\\n')\n",
    "print(logreg.predict_proba(glass['al'][22].reshape(1, -1))[0])\n",
    "print(logreg.predict_proba(glass['al'][185].reshape(1, -1))[0])\n",
    "print(logreg.predict_proba(glass['al'][164].reshape(1, -1))[0])\n",
    "first_row = glass['al'][22].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# store the predicted probabilites of class 1\n",
    "glass['household_pred_prob'] = logreg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "toc": true
   },
   "outputs": [],
   "source": [
    "# plot the predicted probabilities\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(glass.al, glass.household)\n",
    "ax.plot(glass.al, glass.household_pred_prob, color='red')\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cost Functions and Solutions To the Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unlike the least-squares problem for linear regression, no one has yet found a closed-form solution to the optimization problem presented by logistic regression. But even if one exists, the computation would no doubt be so complex that we'd be better off using some sort of approximation method instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But there's still a problem.\n",
    "\n",
    "Recall the cost function for linear regression: <br/><br/>\n",
    "$SSE = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i(y_i - (\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in}))^2$.\n",
    "\n",
    "This function, $SSE(\\vec{\\beta})$, is convex.\n",
    "\n",
    "If we plug in our new logistic equation for $\\hat{y}$, we get: <br/><br/>\n",
    "$SSE_{log} = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i\\left(y_i - \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in})}}\\right)\\right)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Bad News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*This* function, $SSE_{log}(\\vec{\\beta})$, is [**not** convex](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c).\n",
    "\n",
    "That means that, if we tried to use gradient descent or some other approximation method that looks for the minimum of this function, we could easily find a local rather than a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note that the scikit-learn class *expects the user to specify the solver* to be used in calculating the coefficients. The default solver, [lbfgs](https://en.wikipedia.org/wiki/Limited-memory_BFGS), works well for many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Good News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use **log-loss** instead:\n",
    "\n",
    "$\\mathcal{L}(\\vec{y}, \\hat{\\vec{y}}) = -\\frac{1}{N}\\Sigma^N_{i=1}\\left(y_iln(\\hat{y}_i)+(1-y_i)ln(1-\\hat{y}_i)\\right)$,\n",
    "\n",
    "where $\\hat{y}_i$ is the probability that $(x_{i1}, ... , x_{in})$ belongs to **class 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of the return on our `predict_proba()` method indicates the predicted probability of **class 0**, and the second column indicates the predicted probability of **class 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(glass.household, logreg.predict_proba(X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = logreg.predict_proba(X)[:, 1]\n",
    "\n",
    "-np.mean([y*np.log(y_hat) + (1-y)*np.log(1-y_hat)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a pretty good score. A baseline classifier that is fit on data with equal numbers of data points in the two target classes should be right about 50% of the time, and the log loss for such a classifier would be $-ln(0.5) = 0.693$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**More resources on the log-loss function**:\n",
    "\n",
    "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ðŸ§  Knowledge Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Is a bigger value (more positive) better or worse than a smaller value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- What would the log-loss for one data point when the target is $0$ but we predict $1$?\n",
    "\n",
    "- What would the log-loss for one data point when the target is $0$ but we predict $0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Putting It All Together: Training Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's take some time to show how you can do use logistic regression in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note we've been talking about _binary classification_ but we can also do classification for _multiclass_ problems (more than binary classes).\n",
    ">\n",
    "> That's what we'll do for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Built in dataset from sklearn\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=np.c_[iris['data'], iris['target']],\n",
    "    columns=iris['feature_names'] + ['target']\n",
    ")\n",
    "\n",
    "display(df.head())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note how many different targets there are\n",
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can go ahead and explore some graphs to show that it doesn't make sense to do a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating a large figure\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Iterating over the different\n",
    "for i in range(0, 4):\n",
    "    # Figure number starts at 1\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    # Add a title to make it clear what each subplot shows\n",
    "    plt.title(df.columns[i])\n",
    "    # Use alpha to better see crossing pints\n",
    "    ax.scatter(df['target'], df.iloc[:, i], c='teal', alpha=0.1)\n",
    "    # Only show the tick marks for each target\n",
    "    plt.xticks(df.target.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Preparing the data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get the features and then the target\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Normalize the data to help the model\n",
    "X = normalize(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now each row vector has unit length\n",
    "sum([x**2 for x in X[0, :]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Split for test & training  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create and Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There's a lot of different parameters for `LogisticRegression`. Check out the documentation for more info: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_model = LogisticRegression(\n",
    "            C=1e3,             # Smaller values -> more regularization\n",
    "            max_iter=1e3,      # Ensure we eventually reach a solution\n",
    "            solver='lbfgs',    # (Default) Can optimize depending on problem\n",
    "            multi_class='ovr'  # (Default) Will try to do multiclass classification \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fit/Train the model\n",
    "my_model.fit(X_train, y_train)\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Optional: Evaluate the Model with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In practice, we should make this a practice but we skip it if time is running low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "                    estimator=my_model,\n",
    "                    X=X_train,\n",
    "                    y=y_train,\n",
    "                    cv=5,\n",
    "                    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv_results['train_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv_results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cv_overall(cv_results):\n",
    "    val_results = cv_results['test_score']\n",
    "    result_str = f'{val_results.mean():.3f} Â± {val_results.std():.3f}'\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv_overall(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's save these results for later\n",
    "models = {}\n",
    "\n",
    "models['model_1'] = {'model': my_model, 'cv':cv_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Overall Training Score\n",
    "my_model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Optional: Rinse and Repeat - Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try out a few more models for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Adjust the regularization C\n",
    "c_values = [1e-1, 1e2, 1e4, 1e6]\n",
    "\n",
    "# Start at #2 since we have \"model_1\" already\n",
    "for i, c in enumerate(c_values, start=2):\n",
    "    \n",
    "    print(f'Model #{i} with C={c}')\n",
    "    new_model = LogisticRegression(C=c, max_iter=1e3)\n",
    "    \n",
    "    # Cross-validation\n",
    "    print('Cross-validating model with training data...')\n",
    "    cv_results = cross_validate(\n",
    "                    estimator=new_model,\n",
    "                    X=X_train,\n",
    "                    y=y_train,\n",
    "                    cv=5,\n",
    "                    return_train_score=True\n",
    "    )\n",
    "    print(f'\\tCross-Validation Score: {cv_overall(cv_results)}')\n",
    "    \n",
    "    # Train/fit with the full training set\n",
    "    print('Fitting model to full training set...')\n",
    "    new_model.fit(X_train, y_train)\n",
    "    train_score = new_model.score(X_train, y_train)\n",
    "    print(f'\\tScore on training set: {train_score:.3f}')\n",
    "    \n",
    "    # Save results\n",
    "    print('Saving Results...')\n",
    "    models[f'model_{i}'] = {'model': new_model, 'cv': cv_results}\n",
    "    \n",
    "    print('\\n','-'*30,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_model = models['model_5']['model']\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's get predictions for training & testing sets\n",
    "y_hat_train = best_model.predict(X_train)\n",
    "y_hat_test = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Loss on Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_loss(y_train, best_model.predict_proba(X_train)))\n",
    "print(log_loss(y_test, best_model.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Was our model correct?\n",
    "residuals = y_train == y_hat_train\n",
    "\n",
    "print('Number of values correctly predicted:')\n",
    "print(pd.Series(residuals).value_counts())\n",
    "\n",
    "print('\\n','-'*30,'\\n')\n",
    "\n",
    "print('Percentage of values correctly predicted: ')\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "residuals = y_test == y_hat_test\n",
    "\n",
    "print('Number of values correctly predicted:')\n",
    "print(pd.Series(residuals).value_counts())\n",
    "\n",
    "print('\\n','-'*30,'\\n')\n",
    "\n",
    "print('Percentage of values correctly predicted: ')\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Split the data below into train and test, and then convert the y-values (`geysers.kind`) into 1's and 0's. Then use `sklearn` to build a logistic regression model of whether Old Faithful's eruption wait time is long or short, based on the duration of the eruption. Finally, find the points in the test set where the model's prediction differs from the true y-value. How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "geysers = sns.load_dataset('geyser', **{'usecols': ['duration', 'kind']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "geysers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Level Ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Level Up: Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "There are other ways to squeeze the results of a linear regression into the set (0, 1).\n",
    "\n",
    "But the ratio $\\frac{p}{1-p}$ represents the *odds* of some event, where $p$ is the probability of the event.\n",
    "\n",
    "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n",
    "\n",
    "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Dice roll of 1: probability = 1/6, odds = 1/5\n",
    "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n",
    "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n",
    "\n",
    "$$odds = \\frac {probability} {1 - probability}$$\n",
    "\n",
    "$$probability = \\frac {odds} {1 + odds}$$\n",
    "\n",
    "And so the logit function represents the **log-odds** of success (y=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Level Up: Interpreting Logistic Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "How do we interpret the coefficients of a logistic regression? For a linear regression, the situaton was like this:\n",
    "\n",
    "- Linear Regression: We construct the best-fit line and get a set of coefficients. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change in $y$.\n",
    "\n",
    "- Logistic Regression: We find the coefficients of the best-fit line by some approximation method. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change (not in $y$ but) in $ln\\left(\\frac{y}{1-y}\\right)$.\n",
    "\n",
    "We have:\n",
    "\n",
    "$\\ln\\left(\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}\\right) = \\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k$.\n",
    "\n",
    "Exponentiating both sides:\n",
    "\n",
    "$\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)} = e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k}$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right)}\\cdot e^k$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^k\\cdot\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}$\n",
    "\n",
    "That is, the odds ratio at $x_1+1$ has increased by a factor of $e^k$ relative to the odds ratio at $x_1$.\n",
    "\n",
    "For more on interpretation, see [this page](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/binary-logistic-regression/interpret-the-results/all-statistics-and-graphs/coefficients/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# examine the intercept\n",
    "\n",
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "**Interpretation:** For an 'al' value of 0, the log-odds of 'household' is -6.01. What is the probability that glass with an 'al' value of 0 is household glass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert log-odds to probability\n",
    "\n",
    "logodds = logreg.intercept_\n",
    "odds = np.exp(logodds)\n",
    "prob = odds / (1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# examine the coefficient for al\n",
    "\n",
    "list(zip(feature_cols, logreg.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "**Interpretation:** A 1 unit increase in 'al' is associated with a 3.12-unit increase in the log-odds of 'household'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Aside: Verifying log-odds to probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "Let's verify this as we change the aluminum content from 1 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Prediction for al=1\n",
    "\n",
    "pred_al1 = logreg.predict_proba([[1]])\n",
    "pred_al1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Odds ratio for al=1\n",
    "\n",
    "odds_al1 = pred_al1[0][1] / pred_al1[0][0]\n",
    "odds_al1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Prediction for al=2\n",
    "pred_al2 = logreg.predict_proba([[2]])\n",
    "pred_al2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Odds ratio for al=2\n",
    "\n",
    "odds_al2 = pred_al2[0][1] / pred_al2[0][0]\n",
    "odds_al2\n",
    "\n",
    "print((np.exp(logreg.coef_[0]) * odds_al1)[0])\n",
    "print(odds_al2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Aside: Use Coefficients to Generate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compute predicted log-odds for al=2 using the equation\n",
    "x_al = 2\n",
    "logodds = logreg.intercept_ + logreg.coef_[0] * x_al\n",
    "logodds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert log-odds to odds\n",
    "\n",
    "odds = np.exp(logodds)\n",
    "odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert odds to probability\n",
    "\n",
    "prob = odds / (1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compute predicted probability for al=2 using the predict_proba method\n",
    "\n",
    "logreg.predict_proba(np.array([2.0]).reshape(1, 1))[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## More Generalizations: Other Link Functions, Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Logistic regression's link function is the logit function, but different sorts of models use different link functions.\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function) has a nice table of generalized linear model types and their associated link functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
