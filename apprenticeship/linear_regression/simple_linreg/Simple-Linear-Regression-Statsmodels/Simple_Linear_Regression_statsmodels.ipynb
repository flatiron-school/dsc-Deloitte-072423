{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression with `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from random import gauss, seed\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from mpl_toolkits import mplot3d\n",
    "import sklearn.metrics as metrics\n",
    "import statsmodels.api as sm\n",
    "from lin_reg import best_line\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Explain and use the concepts of covariance and correlation\n",
    "- Explain how to use `statsmodels` to construct linear regressions\n",
    "- Explain how to interpret linear regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The idea of _correlation_ is the simple idea that variables often change _together_. For a simple example, cities with more buses tend to have higher populations.\n",
    "\n",
    "We might observe that, as one variable X increases, so does another Y, OR that as X increases, Y decreases.\n",
    "\n",
    "The _covariance_ describes how two variables co-vary. Note the similarity in the definition to the definition of ordinary variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For two variables $X$ and $Y$, each with $n$ values:\n",
    "\n",
    "$\\Large\\sigma_{XY} = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{n}$ <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = [1, 3, 5]\n",
    "Y = [2, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Covariance by hand:\n",
    "((1-3) * (2-7) + (3-3) * (9-7) + (5-3) * (10-7)) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Better yet: With NumPy:\n",
    "np.cov(X, Y, ddof=0)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.cov(X, Y, ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these different numbers? What does `np.cov()` return?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "`np.cov()` returns a covariance *matrix*: variances of individual variables down the main diagonal and covariances of the relevant variables off the main diagonal.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that the value of the covariance is very much a function of the values of X and Y, which can make interpretation difficult. What is often wanted is a _standardized_ scale for covariance, hence: _correlation_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pearson Correlation:<br/>$\\Large r_P = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\Sigma^n_{i = 1}(x_i - \\mu_x)^2\\Sigma^n_{i = 1}(y_i -\\mu_y)^2}}$\n",
    "\n",
    "Note that we are simply standardizing the covariance by the standard deviations of X and Y (the $n$'s cancel!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\bf{Check}$:\n",
    "\n",
    "<details><summary>\n",
    "What happens if X = Y?\n",
    "</summary>\n",
    "Then numerator = denominator and the correlation = 1!\n",
    "</details>\n",
    "<br/>\n",
    "We'll always have $-1 \\leq r \\leq 1$. (This was the point of standardizing by the standard deviations of X and Y.)\n",
    "\n",
    "A correlation of -1 means that X and Y are perfectly negatively correlated, and a correlation of 1 means that X and Y are perfectly positively correlated.\n",
    "\n",
    "NumPy also has a correlation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "4 / np.sqrt(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)[0, 1] == (np.cov(X, Y, ddof=0) / (np.std(X) * np.std(Y)))[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And so does SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats.pearsonr(X, Y)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_Why_ does it happen that variables correlate? It _may_ be that one is the cause of the other. A city having a high population, for example, probably does have some causal effect on the number of buses that the city has. But this _need not_ be the case, and that is why statisticians are fond of saying that 'correlation is not causation'. An alternative possibility, for example, is that high values of X and Y are _both_ caused by high values of some third factor Z. The size of children's feet, for example, is correlated with their ability to spell, but this is of course NOT because either is a cause of the other. Rather, BOTH are caused by the natural maturing and development of children. As they get older, both their feet and their spelling abilities grow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Statistical Learning Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's important at this point to understand the distinction between dependent and independent variables.\n",
    "\n",
    "Roughly, the independent variable is what can be directly manipulated and the dependent variable is what cannot be (but is nevertheless of great interest). What matters structurally is simply that we understand the dependent variable to be a _function_ of the independent variable(s).\n",
    "\n",
    "This is the proper interpretation of a statistical _model_.\n",
    "\n",
    "Simple idea: We can model correlation with a _line_. As one variable changes, so does the other.\n",
    "\n",
    "This model has two *parameters*: *slope* and *y-intercept*.\n",
    "\n",
    "Unless there's a perfectly (and suspiciously) linear relationship between our predictor(s) and our target, there will  be some sort of **error** or **loss** or **residual**. The best-fit line is constructed by minimizing the sum of the squares of these losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Regression Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The solution for a simple regression best-fit line is as follows:\n",
    "\n",
    "- slope: <br/>$\\Large m = r_P\\frac{\\sigma_y}{\\sigma_x} = \\frac{cov(X, Y)}{var(X)}$\n",
    "\n",
    "- y-intercept:<br/> $\\Large b = \\mu_y - m\\mu_x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details>\n",
    "    <summary>Click here</summary>\n",
    "\n",
    "We demonstrate this by setting the derivative of the loss function, $\\Sigma^n_{i=1}(y_i - (mx_i + b))^2$, equal to 0. **We shall see this calculus trick many times!**\n",
    "\n",
    "For this purpose we consider the loss a function of its optimizing parameters $m$ and $b$. So there are therefore two partial derivatives to consider. (We'll cover this in more depth later in the course.)\n",
    "\n",
    "(i) $\\frac{\\partial}{\\partial b}[\\sum^n_{i=1}(y_i - mx_i - b)^2] = -2\\sum^n_{i=1}(y_i - mx_i - b)$\n",
    "\n",
    "(ii) $\\frac{\\partial}{\\partial m}[\\sum^n_{i=1}(y_i - mx_i - b)^2] = -2\\sum^n_{i=1}x_i\\sum^n_{i=1}(y_i - mx_i - b)$\n",
    "\n",
    "- Let's set the first to 0:\n",
    "\n",
    "$-2\\sum^n_{i=1}(y_i - mx_i - b) = 0$ <br/>\n",
    "$\\sum^n_{i=1}(y_i - mx_i) = \\sum^n_{i=1}b = nb$ <br/>\n",
    "\n",
    "**So:** $\\large b = \\frac{\\sum^n_{i=1}(y_i - mx_i)}{n} = \\mu_y - m\\mu_x$\n",
    "\n",
    "- Let's set the second to 0:\n",
    "\n",
    "$-2\\sum^n_{i=1}x_i\\sum^n_{i=1}(y_i - mx_i - b) = 0$ <br/>\n",
    "$\\sum^n_{i=1}(x_iy_i - mx^2_i - bx_i) = 0$ <br/>\n",
    "\n",
    "- Plugging in our previous result, we have:\n",
    "\n",
    "$\\sum^n_{i=1}x_iy_i - (\\frac{1}{n}\\sum^n_{i=1}y_i - \\frac{m}{n}\\sum^n_{i=1}x_i)\\sum^n_{i=1}x_i - m\\sum^n_{i=1}x^2_i = 0$ <br/>\n",
    "$\\sum^n_{i=1}x_iy_i - \\frac{1}{n}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i + \\frac{m}{n}(\\sum^n_{i=1}x_i)^2 - m\\sum^n_{i=1}x^2_i = 0$ <br/>\n",
    "\n",
    "**So:** $\\large m = \\frac{\\sum^n_{i=1}x_iy_i - \\frac{1}{n}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i}{\\sum^n_{i=1}x^2_i - \\frac{1}{n}(\\sum^n_{i=1}x_i)^2} = \\frac{n\\times(\\frac{1}{n}\\sum^n_{i=1}x_iy_i - \\frac{1}{n^2}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i)}{n\\times(\\frac{1}{n}\\sum^n_{i=1}x^2_i - \\mu^2_x)} = \\frac{cov(X, Y)}{var(X)}$\n",
    "\n",
    "For more on the proof see [here](https://math.stackexchange.com/questions/716826/derivation-of-simple-linear-regression-parameters).\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The output of the simple linear regression algorithm is a pair of parameters: the slope and the y-intercept of the best-fit line through the data.\n",
    "\n",
    "***I therefore have a (more or less crude) MODEL of the phenomenon in question:***\n",
    "\n",
    "Suppose I have a bunch of data about (i) how many cigarettes people smoked in their lifetimes and (ii) how many years those same people lived. If I set my independent variable (\"x\") to be the number of cigarettes smoked and my dependent variable (\"y\") to be the number of years lived, then ***for any deceased person at all I will have a way of estimating the number of years that person lived if I know the number of cigarettes that that person smoked***. This estimate is exactly what the best-fit line gives me.\n",
    "\n",
    "Suppose the parameters of the regression come out to be $\\beta_0 = 100$ years and $\\beta_1 = -1\\times 10^{-4}$ years / cigarette ([in reality](https://www.medicalnewstoday.com/releases/9703#1) these are probably both a bit high).\n",
    "\n",
    "Then we would be modeling the lifespan of human beings according to the number of cigarettes smoked:\n",
    "\n",
    "$Y = \\beta_1\\times n + \\beta_0$,\n",
    "\n",
    "where $Y$ = the number of years (estimated) and $n$ is the number of cigarettes smoked.\n",
    "\n",
    "- If someone smoked 0 cigarettes, then we would estimate that person's lifespan as:\n",
    "\n",
    "$-1\\times 10^{-4}\\times 0 + 100 = 100$ years.\n",
    "\n",
    "- If someone smoked a pack a day for 30 years, that's 20 * 365 * 30 = 219000 cigarettes (never mind about leap years!), so we would estimate that person's lifespan as:\n",
    "\n",
    "$-1\\times 10^{-4}\\times 219000 + 100 = 78.1$ years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using `best_line()`\n",
    "\n",
    "Let's take a look at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_line(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "The best-fit line exists no matter what my data look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_rand = stats.uniform.rvs(size=100)\n",
    "Y_rand = stats.uniform.rvs(size=100)\n",
    "\n",
    "best_line(X_rand, Y_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "Experiment: [Playing with regression line](https://www.desmos.com/calculator/jwquvmikhr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Visualization of Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjusting X so that the intercept term of the best-fit line will be 0\n",
    "X = np.array([1.5, 3.5, 5.5])\n",
    "Y = np.array([2, 9, 10])\n",
    "X_plus_c = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(Y, X_plus_c).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "def sse(m):\n",
    "    # sum of squared errors\n",
    "    line = m*X\n",
    "    err = sum(x**2 for x in [line - model.predict(X_plus_c)])\n",
    "    return sum(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ms = np.linspace(0, 5, 100)\n",
    "ys = [sse(m) for m in ms]\n",
    "\n",
    "ax.set_xlabel('Slope Estimates')\n",
    "ax.set_ylabel('Sum of Squared Errors')\n",
    "ax.plot(ms, ys);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "# Going 3d to plot error as a function of both m and b\n",
    "\n",
    "def new_sse(m, x, b, y):\n",
    "    \"\"\"\n",
    "    This function returns the sum of squared errors for\n",
    "    a target y and a linear estimate mx + b.\n",
    "    \"\"\"\n",
    "    return len(x) * metrics.mean_squared_error(y, m*x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "# Going back to our original example\n",
    "X_sample = np.array([1, 3, 5])\n",
    "Y_sample = np.array([2, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "# This should be our minimum error\n",
    "new_sse(2, X_sample, 1, Y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "ms = np.linspace(-3, 7, 100)\n",
    "bs = np.linspace(-5, 5, 100)\n",
    "\n",
    "X_grid, Y_grid = np.meshgrid(ms, bs)\n",
    "\n",
    "Z = np.array([[new_sse(m, X_sample, b, Y_sample) for m in ms] for b in bs])\n",
    "\n",
    "m_errs = {}\n",
    "for m in ms:\n",
    "    m_errs[m] = new_sse(m, X_sample, 1, Y_sample)\n",
    "print(min(m_errs.values()))\n",
    "for k in m_errs:\n",
    "    if m_errs[k] == min(m_errs.values()):\n",
    "        print(k)\n",
    "\n",
    "b_errs = {}\n",
    "for b in bs:\n",
    "    b_errs[b] = new_sse(2, X_sample, b, Y_sample)\n",
    "print(min(b_errs.values()))\n",
    "for k in b_errs:\n",
    "    if b_errs[k] == min(b_errs.values()):\n",
    "        print(k)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X_grid, Y_grid, Z)\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('y-intercept')\n",
    "ax.set_zlabel('sum of squared errors')\n",
    "plt.title('Error as a function of slope and y-intercept');\n",
    "#plt.savefig('images/surfacePlotSSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X_grid, Y_grid, Z, 200)\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('y-intercept')\n",
    "ax.set_zlabel('sum of squared errors')\n",
    "plt.title('Error as a function of slope and y-intercept');\n",
    "#plt.savefig('images/contourPlotSSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Simple Linear Regression with `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's take a look at how to build a simple linear regression model with `statsmodels`. The `statsmodels` package offers a highly descriptive report of the fit of a regression model. Let's generate a simple regression and then analyze the report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First let's try data that fit a straight line perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(20)\n",
    "y = 3*x + 5\n",
    "\n",
    "test_df = pd.DataFrame({'x': x, 'y':y})\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Regression Without Error in `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statsmodels` OLS class API takes an endogenous (dependent) variable and an exogenous (independent) variable. We also want an intercept term, so we'll use the `add_constant()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The data itself:\\n\", test_df['x'], '\\n', '*'*64)\n",
    "print(\"The data with an extra column of 1's:\\n\", sm.add_constant(test_df['x']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Does this make sense?\n",
    "\n",
    "Instead of setting up the regression $y$ ~ $x$, we're setting up $y$ ~ $x_1$ + $x_2$, where $x_2 = 1$ for all observations.\n",
    "\n",
    "- **Without** the constant, we're looking for a parameter $\\beta_1$ that minimizes the error around $y = \\beta_1x$;\n",
    "- **With** the constant, we're looking for two parameters $\\beta_0$ and $\\beta_1$ that minimize the error around $y = \\beta_1x_1 + \\beta_0x_2 = \\beta_1x_1 + \\beta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog = test_df['y']\n",
    "exog = sm.add_constant(test_df['x'])\n",
    "lin_reg_model = sm.OLS(endog, exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call the `.summary()` method on the fitted model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Regression with Error in `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's add a little noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seed(42)\n",
    "\n",
    "x = np.arange(20)\n",
    "y = np.array([3*pt + 5 + gauss(mu=0, sigma=5) for pt in x])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fitted Model Attributes and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The fitted model has [many](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html) attributes and methods. I'll look at a couple here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.mse_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `.summary()` method returns a `Summary` object with lots of helpful information about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are all these statistics!? Let's say a word about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betas and $p$-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the second of the three tables that compose the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we have here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one row for our input variable (\"x1\") and another for our intercept term (\"const\"). The first column (\"coef\") shows us the betas themselves. Remember that we constructed our data by starting with our independent variable $X$ and then setting $Y = 3X + 5 + noise$. As we can see, the noise influenced the data enough to adjust the optimal $\\beta_1$ to 3.0909 and the optimal $\\beta_0$ to 4.2654.\n",
    "\n",
    "What do these coefficients mean?\n",
    "\n",
    "- The 3.0909 coefficient on x1 means that if we adjust x1 *by one unit*, our model will predict the corresponding y value to increase by 3.0909\n",
    "\n",
    "- The 4.2654 coefficient on const means that our model's prediction will be 4.2654 for an *input of 0*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to the .predict() method takes a value both for const and for x1.\n",
    "# The const value will always be 1 (since we want the model to calculate\n",
    "# beta0*1 + beta1*x).\n",
    "\n",
    "model.predict([1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the value of $\\beta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's $\\hat{y}(0) + \\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t and P>|t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns show $t$-values and $p$-values. The regression is ultimately a grand hypothesis test. What are our hypotheses here? The null hypothesis says that the optimal betas are 0, i.e. that there is no linear relationship between our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### std err, \\[0.025, and 0.975\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns have to do with errors around the calculations of the betas themselves. Why are there errors? Doesn't the calculation of the betas have an exact solution?\n",
    "\n",
    "Well, yes, but `statsmodels` makes *multiple* calculations of the betas using different subsets of the data in a process called **bootstrapping**. The \\[0.025 and 0.975\\] columns show the 95% confidence intervals around the estimated betas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Coefficient of Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Very often a data scientist will calculate $R^2$, the *coefficient of determination*, as a measure of how well the model fits the data.\n",
    "\n",
    "$R^2$ for a model is ultimately a _relational_ notion. It's a measure of goodness of fit _relative_ to a (bad) baseline model. This bad baseline model is simply the horizontal line $y = \\mu_Y$, for dependent variable $Y$.\n",
    "\n",
    "The actual calculation of $R^2$ is: <br/> $\\Large R^2\\equiv 1-\\frac{\\Sigma_i(y_i - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}$.\n",
    "\n",
    "$R^2$ is a measure of how much variation in the dependent variable your model explains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple linear regression model, $R^2$ is equal to the square of the correlation between x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.column_stack((x, y)), rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(model.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Other Regression Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What else do we have in this report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **F-statistic**: The F-test measures the significance of your model relative to a model in which all coefficients are 0, i.e. relative to a model that says there is no correlation whatever between the predictors and the target. <br/><br/>\n",
    "- **Log-Likelihood**: The probability in question is the probability of seeing these data points, *given* the model parameter values. The higher this is, the more our data conform to our model and so the better our fit. AIC and BIC are related to the log-likelihood; we'll talk about those later. <br/><br/>\n",
    "- **coef**: These are the betas as calculated by the least-squares regression. We also have p-values and 95%-confidence intervals. <br/><br/>\n",
    "- **Omnibus**: This is a test for error normality. The probability is the chance that the errors are normally distributed. <br/><br/>\n",
    "- **Durbin-Watson**: This is a test for autocorrelation. We'll return to this topic in a future lecture. <br/><br/>\n",
    "- **Jarque-Bera**: This is another test for error normality. <br/><br/>\n",
    "- **Cond. No.**: The condition number tests for independence of the predictors. Lower scores are better. When the predictors are *not* independent, we can run into problems of multicollinearity. For more on the condition number, see [here](https://stats.stackexchange.com/questions/168259/how-do-you-interpret-the-condition-number-of-a-correlation-matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Many good regression diagnostics are available in** [`statsmodels`](https://www.statsmodels.org/dev/examples/notebooks/generated/regression_diagnostics.html). For more on statsmodels regression statistics, see [here](https://www.accelebrate.com/blog/interpreting-results-from-linear-regression-is-the-data-appropriate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Level Up:  [Anscombe's Quartet](https://www.desmos.com/calculator/paknt6oneh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Why do we care about all these assumption checks? They let's us know if we've run a linear regression when we shouldn't have. Anscombe's Quartet demonstates this by showing four sets of data that are wildly different and problematic, but produce the same regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ans = sns.load_dataset('anscombe')\n",
    "sns.scatterplot(data=ans, x='x', y='y', hue='dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Level Up: `statsmodels.formula.ols()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an alternative way of using `statsmodels` to set up a linear regression, and that is with `sm.formula.ols()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm.formula.ols(formula=\"y ~ x\", data=test_df).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this way of doing things *automatically sets up an intercept column*.\n",
    "\n",
    "That's an advantage, but this method has drawbacks inasmuch as the value for the `data` parameter has to have a certain structure. And in fact, once we start using train-test splits, our data will often *fail* to have that requisite structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Level Up: Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are some theoretical [objections](https://data.library.virginia.edu/is-r-squared-useless/) to using $R^2$ as an evaluator of a regression model.\n",
    "\n",
    "One objection is that, if we add another predictor to our model, $R^2$ can only *increase*! (It could hardly be that with more features I'd be able to account for *less* of the variation in the dependent variable than I could with the smaller set of features.)\n",
    "\n",
    "One improvement is **adjusted $R^2$**: <br/> $\\Large R^2_{adj.}\\equiv 1 - \\frac{(1 - R^2)(n - 1)}{n - m - 1}$, where:\n",
    "\n",
    "- n is the number of data points; and\n",
    "- m is the number of predictors.\n",
    "\n",
    "This can be a better indicator of the quality of a regression model. For more, see [here](https://www.statisticshowto.datasciencecentral.com/adjusted-r2/).\n",
    "\n",
    "Note that Adjusted $R^2$ *can* be negative!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
